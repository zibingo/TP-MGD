# Texture-Preserving Multimodal Fashion Image Editing with Diffusion Models

<div style="display: flex; justify-content: center; align-items: center;">
  <a href='https://huggingface.co/zibingo/TP-MGD' style="margin: 0 2px;">
    <img src='https://img.shields.io/badge/Hugging Face-ckpts-orange?style=flat&logo=HuggingFace&logoColor=orange' alt='huggingface'>
  </a>
  <a href="https://github.com/zibingo/TP-MGD" style="margin: 0 2px;">
    <img src='https://img.shields.io/badge/GitHub-Repo-blue?style=flat&logo=GitHub' alt='GitHub'>
  </a>
</div>

## ğŸ¯ Overview

TP-MGD is a new method for texture-preserving multimodal fashion image editing using diffusion models. The project enables high-quality fashion image generation and editing through an innovative lightweight architecture setup while maintaining fine-grained texture details.

<div align="center">
  <img src="assets/sample_by_model.jpg" width="100%" height="100%"/>
</div>

## âœ… TODO

- [x] Release training code
- [x] Release inference code  
- [x] Release processed datasets
- [x] Release checkpoints to Hugging Face
- [x] Create comprehensive documentation
## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/zibingo/TP-MGD.git
cd TP-MGD
```

**Requirements:**

- Python 3.9+
- PyTorch >= 2.5.0
- CUDA >= 12.4

```bash
pip install diffusers accelerate transformers opencv-python einops wandb open_clip_torch
```

### Download Pre-trained Models

```bash
wget https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.bin 
```

## ğŸ“Š Dataset Setup

### VITON-HD Dataset

1. **Download VITON-HD**: Get the original dataset from [VITON-HD](https://github.com/shadow2496/VITON-HD)
2. **Download MGD multimodal data**: Get additional data from [MGD](https://github.com/aimagelab/multimodal-garment-designer)
3. **Download preprocessed textures**: 
   
   ```bash
   wget https://huggingface.co/zibingo/TP-MGD/resolve/main/vitonhd-texture.zip
   ```
<details>
<summary>Directory Structure:</summary>
```
â”œâ”€â”€ captions.json                (from MGD)
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ agnostic-mask/
â”‚   â”œâ”€â”€ agnostic-v3.2/
â”‚   â”œâ”€â”€ cloth/
â”‚   â”œâ”€â”€ cloth-mask/
â”‚   â”œâ”€â”€ cloth-texture/           (from Ours)
â”‚   â”œâ”€â”€ im_sketch/               (from MGD)
â”‚   â”œâ”€â”€ im_sketch_unpaired/      (from MGD)
â”‚   â”œâ”€â”€ image/
â”‚   â”œâ”€â”€ image-densepose/
â”‚   â”œâ”€â”€ image-parse-agnostic-v3.2/
â”‚   â”œâ”€â”€ image-parse-v3/
â”‚   â”œâ”€â”€ openpose_img/
â”‚   â””â”€â”€ openpose_json/
â”œâ”€â”€ test_pairs.txt
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ agnostic-mask/
â”‚   â”œâ”€â”€ agnostic-v3.2/
â”‚   â”œâ”€â”€ cloth/
â”‚   â”œâ”€â”€ cloth-mask/
â”‚   â”œâ”€â”€ cloth-texture/           (from Ours)
â”‚   â”œâ”€â”€ gt_cloth_warped_mask/
â”‚   â”œâ”€â”€ im_sketch/               (from MGD)
â”‚   â”œâ”€â”€ image/
â”‚   â”œâ”€â”€ image-densepose/
â”‚   â”œâ”€â”€ image-parse-agnostic-v3.2/
â”‚   â”œâ”€â”€ image-parse-v3/
â”‚   â”œâ”€â”€ openpose_img/
â”‚   â””â”€â”€ openpose_json/
â””â”€â”€ train_pairs.txt
```
</details>

### DressCode Dataset

1. **Download DressCode**: Get the original dataset from [DressCode](https://github.com/aimagelab/dress-code)
2. **Download MGD multimodal data**: Get additional data from [MGD](https://github.com/aimagelab/multimodal-garment-designer)
3. **Download preprocessed textures**:
   
   ```bash
   wget https://huggingface.co/zibingo/TP-MGD/resolve/main/dresscode-texture.zip
   ```
<details>
<summary>Directory Structure:</summary>
```
â”œâ”€â”€ dresses/
â”‚   â”œâ”€â”€ dense/
â”‚   â”œâ”€â”€ dresses_cloth-texture/    (from Ours)
â”‚   â”œâ”€â”€ im_sketch/                (from MGD)
â”‚   â”œâ”€â”€ im_sketch_unpaired/       (from MGD)
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ keypoints/
â”‚   â”œâ”€â”€ label_maps/
â”‚   â”œâ”€â”€ test_pairs_paired.txt
â”‚   â”œâ”€â”€ test_pairs_unpaired.txt
â”‚   â””â”€â”€ train_pairs.txt
â”œâ”€â”€ lower_body/
â”‚   â”œâ”€â”€ dense/
â”‚   â”œâ”€â”€ im_sketch/                (from MGD)
â”‚   â”œâ”€â”€ im_sketch_unpaired/       (from MGD)
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ keypoints/
â”‚   â”œâ”€â”€ label_maps/
â”‚   â”œâ”€â”€ lower_body_cloth-texture/ (from Ours)
â”‚   â”œâ”€â”€ test_pairs_paired.txt
â”‚   â”œâ”€â”€ test_pairs_unpaired.txt
â”‚   â””â”€â”€ train_pairs.txt
â”œâ”€â”€ upper_body/
â”‚   â”œâ”€â”€ dense/
â”‚   â”œâ”€â”€ im_sketch/                (from MGD)
â”‚   â”œâ”€â”€ im_sketch_unpaired/       (from MGD)
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ keypoints/
â”‚   â”œâ”€â”€ label_maps/
â”‚   â”œâ”€â”€ test_pairs_paired.txt
â”‚   â”œâ”€â”€ test_pairs_unpaired.txt
â”‚   â”œâ”€â”€ train_pairs.txt
â”‚   â””â”€â”€ upper_body_cloth-texture/  (from Ours)
â”œâ”€â”€ coarse_captions.json           (from MGD)
â”œâ”€â”€ fine_captions.json             (from MGD)
â”œâ”€â”€ multigarment_test_triplets.txt
â”œâ”€â”€ readme.txt
â”œâ”€â”€ test_pairs_paired.txt
â”œâ”€â”€ test_pairs_unpaired.txt
â”œâ”€â”€ test_stitch_map/               (from MGD)
â””â”€â”€ train_pairs.txt
```
</details>
**Configuration:** Set the `dataroot_path` in the YAML files under the `configs/` directory.

## ğŸš€ Usage

### Training

**Single GPU:**
```bash
python train_vitonhd.py
python train_dresscode.py
```

**Multi-GPU**

```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch train_vitonhd.py
CUDA_VISIBLE_DEVICES=0,1 accelerate launch train_dresscode.py
```

### Inference

1. **Download pre-trained weights** from [Hugging Face](https://huggingface.co/zibingo/TP-MGD/tree/main) and place them in the `checkpoints/` directory
2. **Update configuration**: Modify the `resume_state` parameter in the YAML files under `configs/` directory to point to your checkpoint directory

**Single GPU:**

```bash
python inference_vitonhd.py
python inference_dresscode.py
```

**Multi-GPU:**

```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch inference_vitonhd.py
CUDA_VISIBLE_DEVICES=0,1 accelerate launch inference_dresscode.py
```

## ğŸ“ Project Structure

```
TP-MGD/
â”œâ”€â”€ configs/                 # Configuration files
â”œâ”€â”€ checkpoints/             # Pre-trained model weights
â”œâ”€â”€ assets/                  # Sample images
â”œâ”€â”€ train_vitonhd.py         # VITON-HD training script
â”œâ”€â”€ train_dresscode.py       # DressCode training script
â”œâ”€â”€ inference_vitonhd.py     # VITON-HD inference script
â”œâ”€â”€ inference_dresscode.py   # DressCode inference script
â”œâ”€â”€ datasets.py              # Dataset loading utilities
â””â”€â”€ attention_processor.py   # Custom attention mechanisms
```

## ğŸ”§ Configuration

Key configuration parameters in `configs/*.yaml`:

- `dataroot_path`: Path to your dataset
- `resume_state`: Path to checkpoint for inference or resume train

## ğŸ™ Acknowledgments

- Our code is based on [Diffusers](https://github.com/huggingface/diffusers)
- We use [Stable Diffusion v1.5 inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting) as the base model
- Thanks to [VITON-HD](https://github.com/shadow2496/VITON-HD), [DressCode](https://github.com/aimagelab/dress-code), and [MGD](https://github.com/aimagelab/multimodal-garment-designer) for providing the public datasets

## ğŸ“ Contact

For questions and support, please open an issue on GitHub or contact the authors.

---

**â­ If you find this project helpful, please give it a star!**

